{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7048cd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 01:49:07,076 - INFO - Starting PDF ingestion process...\n",
      "2025-08-16 01:49:07,110 - INFO - Using device: cuda\n",
      "C:\\Users\\usEr\\AppData\\Local\\Temp\\ipykernel_18028\\1548072419.py:37: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embed_model = HuggingFaceEmbeddings(\n",
      "C:\\Users\\usEr\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-16 01:49:25,685 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-08-16 01:49:30,662 - INFO - Found 5 PDF files to process\n",
      "Processing PDFs:   0%|          | 0/5 [00:00<?, ?it/s]2025-08-16 01:49:30,666 - INFO - Extracting content from: 23-7-25 Sumet_SEEK-Software Engineer (Full stack_AI).pdf\n",
      "2025-08-16 01:49:30,696 - INFO - Extracted 4717 characters from 23-7-25 Sumet_SEEK-Software Engineer (Full stack_AI).pdf\n",
      "2025-08-16 01:49:30,697 - INFO - Created 6 chunks from 23-7-25 Sumet_SEEK-Software Engineer (Full stack_AI).pdf\n",
      "2025-08-16 01:49:30,697 - INFO - Extracting content from: Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf\n",
      "2025-08-16 01:49:30,748 - INFO - Extracted 54532 characters from Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf\n",
      "2025-08-16 01:49:30,751 - INFO - Created 68 chunks from Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf\n",
      "2025-08-16 01:49:30,751 - INFO - Extracting content from: Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf\n",
      "2025-08-16 01:49:30,871 - INFO - Extracted 170056 characters from Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf\n",
      "2025-08-16 01:49:30,875 - INFO - Created 214 chunks from Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf\n",
      "Processing PDFs:  60%|██████    | 3/5 [00:00<00:00, 14.27it/s]2025-08-16 01:49:30,876 - INFO - Extracting content from: Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf\n",
      "2025-08-16 01:49:30,900 - INFO - Extracted 40438 characters from Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf\n",
      "2025-08-16 01:49:30,902 - INFO - Created 51 chunks from Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf\n",
      "2025-08-16 01:49:30,902 - INFO - Extracting content from: Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf\n",
      "2025-08-16 01:49:30,956 - INFO - Extracted 77364 characters from Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf\n",
      "2025-08-16 01:49:30,959 - INFO - Created 97 chunks from Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf\n",
      "Processing PDFs: 100%|██████████| 5/5 [00:00<00:00, 17.05it/s]\n",
      "2025-08-16 01:49:30,960 - INFO - Total documents created: 436\n",
      "C:\\Users\\usEr\\AppData\\Local\\Temp\\ipykernel_18028\\1548072419.py:153: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n",
      "2025-08-16 01:49:31,611 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Adding to vector store: 100%|██████████| 9/9 [00:03<00:00,  2.50it/s]\n",
      "C:\\Users\\usEr\\AppData\\Local\\Temp\\ipykernel_18028\\1548072419.py:166: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n",
      "2025-08-16 01:49:35,524 - INFO - ✅ All documents successfully ingested and persisted\n",
      "2025-08-16 01:49:35,525 - INFO - Vector store summary:\n",
      "2025-08-16 01:49:35,525 - INFO -   - Collection name: pdf_docs\n",
      "2025-08-16 01:49:35,533 - INFO -   - Total documents: 1302\n",
      "2025-08-16 01:49:35,533 - INFO -   - Database location: chroma_db\n",
      "2025-08-16 01:49:35,581 - INFO - Verification test:\n",
      "2025-08-16 01:49:35,581 - INFO -   - Query: 'what'\n",
      "2025-08-16 01:49:35,582 - INFO -   - Results found: 3\n",
      "2025-08-16 01:49:35,582 - INFO -   - Result 1: Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf (chunk 15)\n",
      "2025-08-16 01:49:35,583 - INFO -   - Result 2: Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf (chunk 15)\n",
      "2025-08-16 01:49:35,583 - INFO -   - Result 3: Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf (chunk 15)\n",
      "2025-08-16 01:49:35,583 - INFO - ✅ PDF ingestion completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PDF Ingestion Script for Chat With PDF System\n",
    "Usage: python ingest_pdfs.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PDF processing\n",
    "import fitz  # PyMuPDF\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PDFIngester:\n",
    "    def __init__(self, pdfs_dir: str = \"./pdfs\", db_dir: str = \"./chroma_db\"):\n",
    "        self.pdfs_dir = Path(pdfs_dir)\n",
    "        self.db_dir = Path(db_dir)\n",
    "        \n",
    "        # Setup embedding model\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "        \n",
    "        self.embed_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "            model_kwargs={\"device\": device},\n",
    "            encode_kwargs={\"normalize_embeddings\": True},\n",
    "        )\n",
    "        \n",
    "        # Setup text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        self.pdfs_dir.mkdir(exist_ok=True)\n",
    "        self.db_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def extract_text_pymupdf(self, pdf_path: Path) -> str:\n",
    "        \"\"\"Extract text using PyMuPDF (better for complex layouts)\"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text = \"\"\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "            doc.close()\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PyMuPDF extraction failed for {pdf_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_text_pypdf(self, pdf_path: Path) -> str:\n",
    "        \"\"\"Extract text using pypdf (fallback method)\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text()\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"pypdf extraction failed for {pdf_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_pdf_content(self, pdf_path: Path) -> str:\n",
    "        \"\"\"Extract text from PDF with fallback methods\"\"\"\n",
    "        logger.info(f\"Extracting content from: {pdf_path.name}\")\n",
    "        \n",
    "        # Try PyMuPDF first\n",
    "        text = self.extract_text_pymupdf(pdf_path)\n",
    "        \n",
    "        # Fallback to pypdf if PyMuPDF fails\n",
    "        if not text.strip():\n",
    "            logger.warning(f\"PyMuPDF failed for {pdf_path.name}, trying pypdf\")\n",
    "            text = self.extract_text_pypdf(pdf_path)\n",
    "        \n",
    "        if not text.strip():\n",
    "            logger.error(f\"Failed to extract text from {pdf_path.name}\")\n",
    "            return \"\"\n",
    "        \n",
    "        logger.info(f\"Extracted {len(text)} characters from {pdf_path.name}\")\n",
    "        return text\n",
    "    \n",
    "    def process_pdf(self, pdf_path: Path) -> List[Document]:\n",
    "        \"\"\"Process a single PDF into document chunks\"\"\"\n",
    "        text = self.extract_pdf_content(pdf_path)\n",
    "        \n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        # Split text into chunks\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        \n",
    "        # Create documents with metadata\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"source\": str(pdf_path),\n",
    "                    \"filename\": pdf_path.name,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"total_chunks\": len(chunks)\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        logger.info(f\"Created {len(documents)} chunks from {pdf_path.name}\")\n",
    "        return documents\n",
    "    \n",
    "    def ingest_pdfs(self):\n",
    "        \"\"\"Main ingestion function\"\"\"\n",
    "        # Find all PDF files\n",
    "        pdf_files = list(self.pdfs_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        if not pdf_files:\n",
    "            logger.warning(f\"No PDF files found in {self.pdfs_dir}\")\n",
    "            logger.info(\"Please place PDF files in the ./pdfs directory\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "        \n",
    "        # Process all PDFs\n",
    "        all_documents = []\n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            documents = self.process_pdf(pdf_path)\n",
    "            all_documents.extend(documents)\n",
    "        \n",
    "        if not all_documents:\n",
    "            logger.error(\"No documents were successfully processed\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Total documents created: {len(all_documents)}\")\n",
    "        \n",
    "        # Create/load vector store\n",
    "        try:\n",
    "            vectorstore = Chroma(\n",
    "                collection_name=\"pdf_docs\",\n",
    "                embedding_function=self.embed_model,\n",
    "                persist_directory=str(self.db_dir)\n",
    "            )\n",
    "            \n",
    "            # Add documents in batches\n",
    "            batch_size = 50\n",
    "            for i in tqdm(range(0, len(all_documents), batch_size), desc=\"Adding to vector store\"):\n",
    "                batch = all_documents[i:i + batch_size]\n",
    "                vectorstore.add_documents(batch)\n",
    "            \n",
    "            # Persist the vector store\n",
    "            vectorstore.persist()\n",
    "            logger.info(\"✅ All documents successfully ingested and persisted\")\n",
    "            \n",
    "            # Print summary\n",
    "            collection = vectorstore._collection\n",
    "            logger.info(f\"Vector store summary:\")\n",
    "            logger.info(f\"  - Collection name: {collection.name}\")\n",
    "            logger.info(f\"  - Total documents: {collection.count()}\")\n",
    "            logger.info(f\"  - Database location: {self.db_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating vector store: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def verify_ingestion(self):\n",
    "        \"\"\"Verify that documents were properly ingested\"\"\"\n",
    "        try:\n",
    "            vectorstore = Chroma(\n",
    "                collection_name=\"pdf_docs\",\n",
    "                embedding_function=self.embed_model,\n",
    "                persist_directory=str(self.db_dir)\n",
    "            )\n",
    "            \n",
    "            # Test search\n",
    "            test_query = \"what\"\n",
    "            results = vectorstore.similarity_search(test_query, k=3)\n",
    "            \n",
    "            logger.info(f\"Verification test:\")\n",
    "            logger.info(f\"  - Query: '{test_query}'\")\n",
    "            logger.info(f\"  - Results found: {len(results)}\")\n",
    "            \n",
    "            for i, doc in enumerate(results):\n",
    "                logger.info(f\"  - Result {i+1}: {doc.metadata.get('filename', 'Unknown')} \"\n",
    "                          f\"(chunk {doc.metadata.get('chunk_id', 'N/A')})\")\n",
    "            \n",
    "            return len(results) > 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Verification failed: {e}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    logger.info(\"Starting PDF ingestion process...\")\n",
    "    \n",
    "    # Initialize ingester\n",
    "    ingester = PDFIngester()\n",
    "    \n",
    "    # Check if PDFs directory exists and has files\n",
    "    if not ingester.pdfs_dir.exists():\n",
    "        logger.error(f\"PDFs directory does not exist: {ingester.pdfs_dir}\")\n",
    "        logger.info(\"Please create ./pdfs directory and add your PDF files\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Run ingestion\n",
    "    ingester.ingest_pdfs()\n",
    "    \n",
    "    # Verify ingestion\n",
    "    if ingester.verify_ingestion():\n",
    "        logger.info(\"✅ PDF ingestion completed successfully!\")\n",
    "    else:\n",
    "        logger.error(\"❌ PDF ingestion verification failed\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ab74ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 01:54:38,773 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.tools import tool\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import TypedDict, Optional\n",
    "# ----------------- Embedding Model -----------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "# ----------------- Vector Search Tool -----------------\n",
    "@tool\n",
    "def vector_search(query: str) -> tuple[str, float]:\n",
    "    \"\"\"Searches vector DB for relevant documents.\"\"\"\n",
    "    db = Chroma(\n",
    "        collection_name=\"pdf_docs\",\n",
    "        persist_directory=\"./chroma_db\",\n",
    "        embedding_function=embed_model\n",
    "    )\n",
    "    # docs = db.similarity_search(query, k=2)\n",
    "    docs_with_scores = db.similarity_search_with_score(query, k=2)\n",
    "    if docs_with_scores:\n",
    "        context = \"\\n\\n\".join(\n",
    "            [f\"Document: {doc.metadata.get('source', 'Unknown')}\\nContent: {doc.page_content}\" for doc, _ in docs_with_scores]\n",
    "    )\n",
    "    score = docs_with_scores[0][1]  # เอา similarity score จริง\n",
    "    return context, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940541b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Search Result ===\n",
      "Similarity Score: 0.6662375926971436\n",
      "Context:\n",
      "Document: pdfs\\23-7-25 Sumet_SEEK-Software Engineer (Full stack_AI).pdf\n",
      "Content: S O F T W A R E  E N G I N E E R / A I  E N G I N E E E R\n",
      "SUMET SUANSAMRAN\n",
      "4353/46 Bangna Sukhumvit Rd. Bangkok 10260 | sumetssr@gmail.com | 082-991-6579\n",
      "E D U C A T I O N\n",
      "King Mongkut's Institute of Technology Ladkrabang \n",
      "(2020-2024)\n",
      "Bachelor of Engineering - Computer Engineering\n",
      "GPA :3.38     (Second Class Honors)\n",
      "Ratwinit Bangkeao School \n",
      "(2014-2020)\n",
      "Science & Math Program\n",
      "GPA : 3.5\n",
      "E X P E R I E N C E\n",
      "KBTG (Kasikorn Business-Technology Group)\n",
      "Software & AI Engineer | 2025 – Present\n",
      "Auto-Approval Service\n",
      "Designed and deployed a scalable microservice for automatic document approval using Google Cloud Run\n",
      "Enabled real-time decisions, reducing manual processing workload by over 70%\n",
      "Document Complexity Classification (YOLOv10)\n",
      "Trained YOLOv10 model to classify document complexity (e.g., simple vs. structured)\n",
      "Achieved >90% accuracy using custom-labeled datasets and advanced augmentation\n",
      "Document Processing Web App\n",
      "\n",
      "Document: pdfs/23-7-25 Sumet_SEEK-Software Engineer (Full stack_AI).pdf\n",
      "Content: S O F T W A R E  E N G I N E E R / A I  E N G I N E E E R\n",
      "SUMET SUANSAMRAN\n",
      "4353/46 Bangna Sukhumvit Rd. Bangkok 10260 | sumetssr@gmail.com | 082-991-6579\n",
      "E D U C A T I O N\n",
      "King Mongkut's Institute of Technology Ladkrabang \n",
      "(2020-2024)\n",
      "Bachelor of Engineering - Computer Engineering\n",
      "GPA :3.38     (Second Class Honors)\n",
      "Ratwinit Bangkeao School \n",
      "(2014-2020)\n",
      "Science & Math Program\n",
      "GPA : 3.5\n",
      "E X P E R I E N C E\n",
      "KBTG (Kasikorn Business-Technology Group)\n",
      "Software & AI Engineer | 2025 – Present\n",
      "Auto-Approval Service\n",
      "Designed and deployed a scalable microservice for automatic document approval using Google Cloud Run\n",
      "Enabled real-time decisions, reducing manual processing workload by over 70%\n",
      "Document Complexity Classification (YOLOv10)\n",
      "Trained YOLOv10 model to classify document complexity (e.g., simple vs. structured)\n",
      "Achieved >90% accuracy using custom-labeled datasets and advanced augmentation\n",
      "Document Processing Web App\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query = \"Explain resume of sumet suansamran\"\n",
    "    context, score = vector_search(query)\n",
    "    \n",
    "    print(\"=== Search Result ===\")\n",
    "    print(f\"Similarity Score: {score}\")\n",
    "    print(f\"Context:\\n{context}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
